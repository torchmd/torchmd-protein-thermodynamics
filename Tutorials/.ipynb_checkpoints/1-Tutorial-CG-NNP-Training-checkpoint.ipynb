{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NNP\n",
    "\n",
    "In this tutorial, we provide instructions on training a coarse-grained model using torchmd-net.\n",
    "To execute the training of NNP install `torchmd-net` repo: https://github.com/torchmd/torchmd-net\n",
    "\n",
    "# Training Data\n",
    "\n",
    "The arrays conataing coorinates and forces for CA atoms of the proteins described in the papers can be downloaded in `../Datasets`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we will follow a reduced exmaple of the smallest protein Chignolin. First download the files:\n",
    "\n",
    "Coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget pub.htmd.org/protein_thermodynamics_data/training_data/chignolin_ca_coords.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And delta-forces. Delta-forces are produced by substraction of prior forces form true forces produced during the MD runs. The prior forces are computed based on `Bonded, RepusionCG, Dihedral` force terms. For details on functional form of these force terms look in Supporting Information of the publication. The prior force field used for computation are provided in `prior_force_firld.yaml`. For a tutorial on how to construct the forcefield and compute delta forces look in torchmd-cg repo. https://github.com/torchmd/torchmd-cg\n",
    "\n",
    "\n",
    "Here we provide already computed delta-forces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget pub.htmd.org/protein_thermodynamics_data/training_data/chignolin_ca_deltaforces.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Aminoacids are encoded as integers. The embedding dictionary we used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA2INT = {'ALA':1,\n",
    "         'GLY':2,\n",
    "         'PHE':3,\n",
    "         'TYR':4,\n",
    "          'ASP':5,\n",
    "          'GLU':6,\n",
    "          'TRP':7,\n",
    "          'PRO':8,\n",
    "          'ASN':9,\n",
    "          'GLN':10,\n",
    "          'HIS':11,\n",
    "          'HSE':11,\n",
    "          'HSD':11,\n",
    "          'SER':12,\n",
    "          'THR':13,\n",
    "          'VAL':14,\n",
    "          'MET':15,\n",
    "          'CYS':16,\n",
    "          'NLE':17,\n",
    "          'ARG':19,\n",
    "          'LYS':20,\n",
    "          'LEU':21,\n",
    "          'ILE':22\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget pub.htmd.org/protein_thermodynamics_data/training_data/chignolin_ca_embeddings.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we use coordinates and delta-forces to train the network. \n",
    "\n",
    "SchNet architecture, applied here, learns the features using continuous filter convolutions on a graph neural network and predicts the forces and energy of the system. \n",
    "\n",
    "A set of parameters in the configuration file `train.yaml` is listed here:\n",
    "\n",
    "```yaml\n",
    "activation: tanh\n",
    "batch_size: 256\n",
    "inference_batchsize: 256\n",
    "dataset: Custom\n",
    "coord_files: \"chignolin_ca_coords.npy\"\n",
    "embed_files: \"chignolin_ca_embeddings.npy\"\n",
    "force_files: \"chignolin_ca_deltaforces.npy\"\n",
    "cutoff_upper: 12.0\n",
    "cutoff_lower: 3.0\n",
    "derivative: true\n",
    "distributed_backend: ddp\n",
    "early_stopping_patience: 30\n",
    "embedding_dimension: 128\n",
    "label:\n",
    "- forces\n",
    "lr: 0.0005\n",
    "lr_factor: 0.8\n",
    "lr_min: 1.0e-06\n",
    "lr_patience: 10\n",
    "lr_warmup_steps: 0\n",
    "model: graph-network\n",
    "neighbor_embedding: false\n",
    "ngpus: -1\n",
    "num_epochs: 100\n",
    "num_layers: 4\n",
    "num_nodes: 1\n",
    "num_rbf: 18\n",
    "num_workers: 8\n",
    "rbf_type: expnorm\n",
    "save_interval: 2\n",
    "seed: 1\n",
    "test_interval: 2\n",
    "test_ratio: 0.1\n",
    "trainable_rbf: true\n",
    "val_ratio: 0.05\n",
    "weight_decay: 0.0\n",
    "```\n",
    "\n",
    "In case on training on multiple protein datasets use:\n",
    "\n",
    "```yaml\n",
    "coord_files: \"data/*coords*.npy\"\n",
    "embed_files: \"data/*embeddings.npy\"\n",
    "force_files: \"data/*deltaforces*.npy\"\n",
    "```\n",
    "\n",
    "Now we will go through options in a configuration file:\n",
    "\n",
    "* training input files locations are defined in parameters: `coords`, `forces` and `embeddings`\n",
    "* `log_dir` - output folder\n",
    "* `lr` - initial value of learning rate \n",
    "* `num_epochs` - number of epochs run during the training\n",
    "* `batch_size` - batch size\n",
    "* `lr` - initial value of learning rate. The learning rate is optimized with `torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler with parameters: `lr_patience`, `lr_min` and `lr_factor`\n",
    "* `distributed_backend` - specifies distributed_backend pytorch-ligtning. Here `dp` (Data Parallel) is adjusted for training on multiple-gpus (`gpus`) and 1 machine (`num_nodes`). Other options include:\n",
    "    * Data Parallel (`distributed_backend='dp'`)(multiple-gpus, 1 machine)\n",
    "    * DistributedDataParallel (`distributed_backend=’ddp’`) (multiple-gpus across many machines (python script based)).\n",
    "    * DistributedDataParallel (`distributed_backend=’ddp_spawn’`) (multiple-gpus across many machines (spawn based)).\n",
    "    * DistributedDataParallel 2 (`distributed_backend=’ddp2’`) (DP in a machine, DDP across machines).\n",
    "    * Horovod (`distributed_backend=’horovod’`) (multi-machine, multi-gpu, configured at runtime)\n",
    "* `gpus` - number of GPUs used in training. Specified as a number of required units (eg. `4`) or a list of cuda devices (eg. `[0, 2, 3]')\n",
    "* `num_nodes` - number of machines used\n",
    "* `num_workers` - number of workers in data loader\n",
    "* `seed` for the calculation\n",
    "* `eval_interval` - evaluation interval\n",
    "* `save_interval` - saving interval\n",
    "* `progress` - Progress bar during batching\n",
    "* `val_ratio` - Percentual of validation set\n",
    "* `test_ratio` - Percentual of test set\n",
    "* Finally schnet-specific parameters: \n",
    "    * `num_filters`\n",
    "    * `num_gaussians`\n",
    "    * `num_interactions`\n",
    "    * `max_z`\n",
    "    * `cutoff`\n",
    "\n",
    "Training is done using python script and can be run by a simple command:\n",
    "\n",
    "```bash\n",
    "mkdir train_light\n",
    "CUDA_VISIBLE_DEVICES=0 python $PATH/torchmdnet/scripts/train.py --conf train.yaml --log-dir train_light\n",
    "```\n",
    "where `$PATH` is the path to your `torchmd-net` repo.\n",
    "\n",
    "The training saves 8 best epochs. The progress of the training is saved in TensorBoard session. The training and validation curves for the training of full Chignolin dataset are presented here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
